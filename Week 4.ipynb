{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a discussion of distributions. And we'll start with the most common using a coin. When a coin is flipped it has a probability of landing heads up and a probability of landing tails up. If we flip a coin many times we collect a number of measurements of the heads and tails that landed face up and intuitively we know that the number of times we get a heads up will be equal about the number of times we get a tails up for a fair coin. If you flipped a coin a hundred times and you received heads each time you'd probably doubt the fairness of that coin. We can consider the result of each flip of this coin as a **random variable.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we can consider the set of all possible random variables together we call this a **distribution**. In this case the distribution is called **binomial** since there's two possible outputs a heads or a tails. It's also an example of a **discreet distribution** since there are only categories being used a heads and a tails and not real numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"distributions.png\" title=\"Distributions\" alt=\"Distribution png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NumPy actually has some distributions built into it allowing us to make random flips of a coin with given parameters. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we ask for a number from the NumPy binomial distribution. We have two parameters to pass in: the first is the number of times we want it to run. The second is the chance we get a zero, which we will use to represent heads here. Let's run one round of this simulation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we run the simulation a thousand times and divided the result by a thousand. Well you see a number pretty close to 0.5 which means half of the time we had a heads and half of the time we had a tails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1000, 0.5)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to simulate the probability of flipping a fair coin 20 times, and getting a number greater than or equal to 15. We can use **np.random.binomial(n,p,size) to do 10000 simulations of flipping a fair coin 20 times**, then see what proportion of the simulations are 15 or greater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009\n"
     ]
    }
   ],
   "source": [
    "x = np.random.binomial(20, .5, 1000)\n",
    "print((x>=15).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course an **even weighted binomial distribution is only one simple example**. We can also have **unevenly weighted binomial distributions**. For instance what's the chance although we're tornado today while I’m filming. It's pretty low even though we do get tornadoes here. So maybe there a hundredth of a percentage chance. We can put this into a binomial distribution as a weighting in NumPy. If we run this 100,000 times we see there are pretty minimal tornado events. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chance_of_tornado = 0.01/100\n",
    "np.random.binomial(100000, chance_of_tornado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational tools allow us to simulate the world which helps us answer questions. A simulation is essentially another form of inquiry. Let's take one more example. Let's say the chance of a tornado here in Ann Arbor on any given day, is 1% regardless of the time of year. That's higher than realistic but it makes for a quicker demo. And lets say if there's a tornado I'm going to get away from the windows and hide, then come back and do my recording the next day. So what's the chance of this happening two days in a row?\n",
    "\n",
    "We can use the binomial distribution in NumPy to simulate this. We start by creating an empty list and then we create a number of potential tornado events by asking the NumPy binomial function using our chance of tornado. We'll do this a million times which is just shy of 3,000 years worth of events. This process is called **sampling the distribution.**\n",
    "\n",
    "Now we can write a little loop to go through the list and look for any two adjacent pairs of ones which means that there were two days that had back to back tornadoes. We see that this ends up being roughly 102 day tornado events over the 3,000 years. Which frankly is still too many for me. My point here though is that modern computational power allows us to very quickly simulate the effects of different parameters in a distribution. Leading to a new way of understanding the problem. You don't have to work out all the math you can quite often simulate the problem instead and observe the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 tornadoes back to back in 2739.72602739726 years\n"
     ]
    }
   ],
   "source": [
    "chance_of_tornado = 0.01\n",
    "\n",
    "tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)\n",
    "    \n",
    "two_days_in_a_row = 0\n",
    "for j in range(1,len(tornado_events)-1):\n",
    "    if tornado_events[j]==1 and tornado_events[j-1]==1:\n",
    "        two_days_in_a_row+=1\n",
    "\n",
    "print('{} tornadoes back to back in {} years'.format(two_days_in_a_row, 1000000/365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the distributions you use in data science are not discrete binomial, and instead are **continuous, where the value of the given observation isn't a category like heads or tails, but can be represented as a real number**. It's common to then graph these distributions when talking about them, where the x axis is the value of the observation and the y axis represents the probability that a given observation will occur. \n",
    "\n",
    "If all numbers are equally likely to be drawn when you sample from it, this should be graphed as a flat horizontal line. And this flat line is actually called the **uniform distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"uniform_distribution_continuous.png\" title=\"Uniform Distributions\" alt=\"Uniform Distribution png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few other distributions that get a lot more interesting. Let's take the **normal distribution** which is also called the **Gaussian Distribution** or sometimes, a **Bell Curve**. This distribution looks like a hump, where the number which has the highest probability of being drawn is a zero, and there are two decreasing curves on either side of the X axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"normal_distribution.png\" title=\"Normal Distributions\" alt=\"Normal Distribution png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the properties of this distribution is that the **mean is zero**, and that the two curves on either side are symmetric. I want to introduce you to the term **expected value**. I think that most of us are familiar with the mean is the sum of all the values divided by the total number of values. Calculating a mean value is a computational process, and it takes place by looking at samples from distribution. For instance rolling a die three times might give you the numbers 1, 2 and 6, the mean value is then 4.5. **The expected value is the probability from the underlying distribution**. It is what would be the mean of a die roll if we did an infinite number of rolls. The result is 3.5 since each face of the die is equally likely to appear. Thus the expected value is 3.5, while the mean value depends upon the samples that we've taken, and converges to the expected value given a sufficiently large sample set. A second property is that the **variance of the distribution** can be described in a certain way. The variance is **a measure of how badly values of samples are spread out from the mean**. Let's get a little bit more formal about five different characteristics of distributions. First, we can talk about the **distribution’s central tendency**, and the measures we would use for this are **mode, median, or mean**. This characteristic is really about **where the bulk of probability is in the distribution**. We can also talk about the **variability in the distribution**. There are a couple of ways we can speak of this. **The standard deviation** is one, the **interquartile range** is another. **The standard deviation is simply a measure of how different each item, in our sample is from the mean.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41277130665579875"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7082862672181012"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula for standard deviation\n",
    "$$\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might look a little more intimidating than it actually is. Let's just walk through how we would write this up. Let's draw 1,000 samples from a normal distribution with an expected value of 0.75 and a standard deviation of 1. Then we calculate the actual mean using NumPy's mean feature. The part inside the summation says xi - xbar. **xi is the current item in the list and xbar is the mean**. So we calculate the difference, then we square the result, then we sum all of these.\n",
    "\n",
    "This might be a reasonable place to use a map and apply a lambda to calculate the differences between the mean and the measured value, then to convert this back to a list, so NumPy can use it. Now we just have to square each value, sum them together, and take the square root. So that's **the size of our standard deviation. It covers roughly 68% of the area around the mean, split evenly around the side of the mean**. Now we don't normally have to do all this work ourselves, but I wanted to show you how you can sample from the distribution, create a precise programmatic description of a formula, and apply it to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811898138480224"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = np.random.normal(0.75,size=1000)\n",
    "\n",
    "np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For standard deviation, which is just one particular measure of variability, NumPy has a built-in function that you can apply, called **std**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811898138480224"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a couple more measures of distributions that are interesting to talk about. One of these is **the shape of the tales of the distribution** and this is called the **kurtosis**. We can measure the kurtosis using the statistics functions in the **SciPy package**. A **negative value means the curve is slightly more flat than a normal distribution, and a positive value means the curve is slightly more peaky than a normal distribution**. Remember that **we aren't measuring the kurtosis of the distribution per se, but of the thousand values which we sampled out of the distribution**. This is a sublet but important distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19300083724947958"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.kurtosis(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also move out of the normal distributions and push the peak of the curve one way or the other. And this is called the **skew**.If we test our current sample data, we see that there isn't much of a skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0003354557069775936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.skew(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a distribution called the **Chi Squared distribution**, which is also quite commonly used in statistics. The Chi Squared Distribution has only one parameter called the **degrees of freedom**. The degrees of freedom is closely related to the **number of samples that you take from a normal population**, and it's important for significance testing. But what I would like you to observe, is that **as the degrees of freedom increases, the shape of the Chi Squared distribution changes**. In particular, the skew to the left begins to move towards the center. We can observe this through simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"chi-squared-distribution.png\" title=\"Chi Squared Distribution\" alt=\"chi-squared-distribution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll sample 1,000 values from a Chi Squared distribution with degrees of freedom 2. Now we can see that the skew is quite large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1136699991722754"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df2 = np.random.chisquare(2, size=10000)\n",
    "stats.skew(chi_squared_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we resample changing degrees of freedom to 5. We see that the skew has decreased significantly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3163138967396069"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df5 = np.random.chisquare(5, size=10000)\n",
    "stats.skew(chi_squared_df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can actually plot this right in the Jupyter notebook. You can see a histogram with our plot with the two degrees of freedom is skewed much further to the left, while our plot with the five degrees of freedom is not as highly skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb1ac19ef98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAGlpJREFUeJzt3X9wldW97/H3t8AlDESwgogJGGwRQYL8CCgDg2hVOFBF\npFrRURypaIuttGeo4LSVW4cOPWhRaUXwaouoIJZaqeAVf3ClMJcCWo4IHAoDqMlQgqhA0HBI+J4/\n9kO6Cdn5uZO9d9bnNbMnz17Pj70enrA/edZae21zd0REJExfS3UFREQkdRQCIiIBUwiIiARMISAi\nEjCFgIhIwBQCIiIBUwiIiARMISAiEjCFgIhIwFqmugI16dixo+fl5aW6GiIiGeW999771N071bRd\n2odAXl4emzdvTnU1REQyipl9VJvt1BwkIhIwhYCISMAUAiIiAUv7PgGREJw4cYLCwkJKS0tTXRXJ\nMFlZWeTm5tKqVat67a8QEEkDhYWFZGdnk5eXh5mlujqSIdydQ4cOUVhYSPfu3et1DDUHiaSB0tJS\nzjnnHAWA1ImZcc455zToDlIhIJImFABSHw39vVEIiIgETH0CIulobj4c/jh5x2vfDX68tdpN8vLy\nyM7OpkWLFrRs2bJWH9Js164dJSUlyaplk3n55Zf5xS9+wXnnnceaNWtOWzdt2jRWrVrF6NGjmTNn\nTlJf98477+Tb3/423/nOd5J63IZo1iEwdPY7FH3x1RnlOR3asH76VSmokUgtHf4YZh5O3vFmtq/V\nZmvWrKFjx47Je91aKisro2XLpns7euaZZ3j66acZNmzYGesWLlzIZ599RosWLU4rb+o6NpVm3RxU\n9MVX7Js95oxHVcEgIjXbu3cvQ4YMIT8/n5/97GenrZszZw6DBg2ib9++PPTQQxXlDz/8MD179mTY\nsGFMmDCBRx55BIARI0YwdepUCgoKePzxxzl48CDjx49n0KBBDBo0iPXr1wNw7Ngx7rrrLgYPHkz/\n/v159dVXAdi2bRuDBw+mX79+9O3bl127dp1R3yVLlpCfn0+fPn144IEHAPjlL3/JunXrmDRpEtOm\nTTtt++uvv56SkhIGDhzISy+9xJ133sm9997LZZddxk9/+tOEdSkvL2fatGkV579gwQIgNnrnvvvu\no2fPnlx99dUUFxdXvNbbb79N//79yc/P56677uL48eNA7I5sxowZ9OvXj4KCAt5//31GjhzJN77x\nDZ566qn6X7xE3D2tHwMHDvT6uuCB1+pULpIq27dvP73gobOS+wK1OF5eXp5feumlPmDAAF+wYEGV\n21x33XW+aNEid3f/7W9/623btnV39zfeeMPvvvtuP3nypJeXl/uYMWP83Xff9Y0bN/qll17qX331\nlR85csS/+c1v+pw5c9zd/YorrvDvf//7FceeMGGC//Wvf3V3948++sgvvvhid3efMWOGL1682N3d\nP//8c+/Ro4eXlJT4fffd588//7y7ux8/fty//PLL0+paVFTkXbt29eLiYj9x4oRfeeWV/sorr1S8\n9qZNm6o8x1Pn5O4+ceJEHzNmjJeVlVVblwULFvjDDz/s7u6lpaU+cOBA37Nnjy9fvtyvvvpqLysr\n86KiIm/fvr2//PLL/tVXX3lubq7v3LnT3d1vv/12nzt3rru7X3DBBf7kk0+6u/vUqVM9Pz/fjxw5\n4sXFxX7uuedWWeczfn/cHdjstXiPbX73NiJSL+vWrSMnJ4fi4mKuueYaLr74YoYPH37aNuvXr2f5\n8uUA3H777RV/Xa9evZrVq1fTv39/AEpKSti1axdHjx5l7NixZGVlkZWVxXXXXXfa8b773e9WLL/1\n1lts37694vmRI0coKSlh9erVrFixouIOorS0lI8//pghQ4Ywa9YsCgsLufHGG+nRo8dpx960aRMj\nRoygU6fYRJq33XYba9eu5YYbbqjTv8tNN91U0TSUqC6rV6/mgw8+4I9//CMAhw8fZteuXaxdu5YJ\nEybQokULzj//fK66KtYMvXPnTrp3785FF10EwMSJE/nd737H1KlTgdgdCUB+fj4lJSVkZ2eTnZ1N\n69at+eKLL+jQoUOdzqE6CgERASAnJweAc889l3HjxrFx48YzQgCqHpLo7syYMYN77rnntPLHHnus\n2tds27ZtxfLJkyfZsGEDWVlZZxx7+fLl9OzZ87TyXr16cdlll7Fy5UpGjx7NggULKt5kkym+jonq\n4u7MmzePkSNHnla+atWqer1m69atAfja175WsXzqeVlZWb2OmUiz7hMQkdo5duwYR48erVhevXo1\nffr0OWO7oUOHsnTpUgBeeOGFivKRI0fy7LPPVowUKioqori4mKFDh/KXv/yF0tJSSkpKeO211xLW\n4dprr2XevHkVz7ds2VJx7Hnz5hFr4YC///3vAOzZs4cLL7yQH/3oR4wdO5YPPvjgtOMNHjyYd999\nl08//ZTy8nKWLFnCFVdcUed/m3iJ6jJy5Ejmz5/PiRMnAPjHP/7BsWPHGD58OC+99BLl5eXs37+/\nYiRSz5492bdvH7t37wZg8eLFDa5bfelOQCQdte9W6xE9tT5eNQ4cOMC4ceOA2CiYW2+9lVGjRp2x\n3eOPP86tt97Kr3/9a8aOHVtRfu2117Jjxw6GDBkCxIaOPv/88wwaNIjrr7+evn370rlzZ/Lz82nf\nvurzeuKJJ5gyZQp9+/alrKyM4cOH89RTT/Hzn/+cqVOn0rdvX06ePEn37t157bXXWLZsGYsXL6ZV\nq1acd955PPjgg6cdr0uXLsyePZsrr7wSd2fMmDGn1bk+EtXle9/7Hvv27WPAgAG4O506deLPf/4z\n48aN45133qF3795069at4t8nKyuL3//+99x0002UlZUxaNAg7r333gbVrb7sVKKlq4KCAq/vl8rk\nTV/Jvtljal0ukio7duygV69eqa5GoygpKaFdu3Z8+eWXDB8+nIULFzJgwIBUV6tZqer3x8zec/eC\nmvbVnYCINKrJkyezfft2SktLmThxogIgzSgERKRRvfjii6muglRDHcMiIgGrMQTMrKuZrTGz7Wa2\nzczuj8pnmlmRmW2JHqPj9plhZrvNbKeZjYwrH2hmW6N1T5imTRQRSanaNAeVAf/u7u+bWTbwnpm9\nGa2b6+6PxG9sZr2BW4BLgPOBt8zsIncvB+YDdwN/A1YBo4DXk3MqIiJSVzXeCbj7fnd/P1o+CuwA\ncqrZZSyw1N2Pu/teYDcw2My6AGe5+4boI83PAXX76J6IiCRVnTqGzSwP6E/sL/mhwA/N7A5gM7G7\nhc+JBcSGuN0Ko7IT0XLlchGpJNEMuPVV08y5n3zyCXfccQcHDhzAzJg8eTL3339/jcfVVNJ1k9FT\nSZtZO2A5MNXdj5jZfOBhwKOfjwJ3JaNSZjYZmAzQrVv1H3IRaY5OzYCbLHnTV1a7vmXLljz66KMM\nGDCAo0ePMnDgQK655hp69+6dtDpUR1NJp06tRgeZWStiAfCCu/8JwN0PuHu5u58EngYGR5sXAV3j\nds+Nyoqi5crlZ3D3he5e4O4FpyZ/EpHG06VLl4rx+9nZ2fTq1YuiojP/e2oq6QCnkgaMWPv9Y5XK\nu8Qt/5hYPwDEOoT/E2gNdAf2AC2idRuBy6Njvg6Mrun1NZW0hKDyVMDJ/h2ty/H27t3rXbt29cOH\nD5+xTlNJhzmV9FDgdmCrmW2Jyh4EJphZP2LNQfuAe6JQ2WZmy4DtxEYWTfHYyCCAHwB/ANpEIaCR\nQSJppKSkhPHjx/PYY49x1llnnbFeU0kHOJW0u68j9pd7ZQnnSHX3WcCsKso3A2dOTSgiKXfixAnG\njx/Pbbfdxo033phwO00lramkRaSZcXcmTZpEr169+MlPfpJwO00lramkRaQJ5HRoU+OInroerzrr\n169n8eLF5Ofn069fPwB+9atfMXr06NO201TSmkq6yWkqaQmBppKWhtBU0iKStjSVdHpTCIhIo9JU\n0ulNHcMiaSLdm2YlPTX090YhIJIGsrKyOHTokIJA6sTdOXTo0BnDautCzUEiaSA3N5fCwkIOHjyY\n6qpIhsnKyiI3N7fmDRNQCIikgVatWtG9e/dUV0MCpOYgEZGAKQRERAKmEBARCZhCQEQkYAoBEZGA\nKQRERAKmEBARCZhCQEQkYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGAKQRERAKmEBARCZhCQEQk\nYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGA1RgCZtbVzNaY2XYz22Zm90flXzezN81sV/Tz7Lh9\nZpjZbjPbaWYj48oHmtnWaN0TZmaNc1oiIlIbtbkTKAP+3d17A5cDU8ysNzAdeNvdewBvR8+J1t0C\nXAKMAp40sxbRseYDdwM9oseoJJ6LiIjUUY0h4O773f39aPkosAPIAcYCi6LNFgE3RMtjgaXuftzd\n9wK7gcFm1gU4y903uLsDz8XtIyIiKVCnPgEzywP6A38DOrv7/mjVP4HO0XIO8EncboVRWU60XLm8\nqteZbGabzWzzwYMH61JFERGpg1qHgJm1A5YDU939SPy66C97T1al3H2huxe4e0GnTp2SdVgREamk\nZW02MrNWxALgBXf/U1R8wMy6uPv+qKmnOCovArrG7Z4blRVFy5XLm1xOhzbkTV9ZZfn66VeloEYi\nIqlRYwhEI3ieAXa4+2/iVq0AJgKzo5+vxpW/aGa/Ac4n1gG80d3LzeyImV1OrDnpDmBe0s6kDhK9\n0VcVDCIizVlt7gSGArcDW81sS1T2ILE3/2VmNgn4CLgZwN23mdkyYDuxkUVT3L082u8HwB+ANsDr\n0UNERFKkxhBw93VAovH830qwzyxgVhXlm4E+damgiIg0Hn1iWEQkYAoBEZGAKQRERAKmEBARCZhC\nQEQkYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGAKQRERAKm\nEBARCZhCQEQkYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGAKQRERAKmEBARCZhCQEQkYAoBEZGA\nKQRERAKmEBARCZhCQEQkYDWGgJk9a2bFZvZhXNlMMysysy3RY3TcuhlmttvMdprZyLjygWa2NVr3\nhJlZ8k9HRETqojZ3An8ARlVRPtfd+0WPVQBm1hu4Bbgk2udJM2sRbT8fuBvoET2qOqaIiDShGkPA\n3dcCn9XyeGOBpe5+3N33AruBwWbWBTjL3Te4uwPPATfUt9IiIpIcDekT+KGZfRA1F50dleUAn8Rt\nUxiV5UTLlctFRCSF6hsC84ELgX7AfuDRpNUIMLPJZrbZzDYfPHgwmYcWEZE49QoBdz/g7uXufhJ4\nGhgcrSoCusZtmhuVFUXLlcsTHX+huxe4e0GnTp3qU0UREamFeoVA1MZ/yjjg1MihFcAtZtbazLoT\n6wDe6O77gSNmdnk0KugO4NUG1FtERJKgZU0bmNkSYATQ0cwKgYeAEWbWD3BgH3APgLtvM7NlwHag\nDJji7uXRoX5AbKRRG+D16CEiIilUYwi4+4Qqip+pZvtZwKwqyjcDfepUuyaW06ENedNXVlm+fvpV\nKaiRiEjjqjEEQpLojb6qYBARaQ40bYSISMAUAiIiAVMIiIgETCEgIhIwhYCISMAUAiIiAVMIiIgE\nTCEgIhIwhYCISMAUAiIiAVMIiIgETCEgIhIwhYCISMAUAiIiAVMIiIgETCEgIhIwhYCISMAUAiIi\nAVMIiIgETCEgIhIwhYCISMAUAiIiAVMIiIgETCEgIhIwhYCISMAUAiIiAVMIiIgETCEgIhKwGkPA\nzJ41s2Iz+zCu7Otm9qaZ7Yp+nh23boaZ7TaznWY2Mq58oJltjdY9YWaW/NMREZG6qM2dwB+AUZXK\npgNvu3sP4O3oOWbWG7gFuCTa50kzaxHtMx+4G+gRPSofU0REmliNIeDua4HPKhWPBRZFy4uAG+LK\nl7r7cXffC+wGBptZF+Asd9/g7g48F7ePiIikSH37BDq7+/5o+Z9A52g5B/gkbrvCqCwnWq5cLiIi\nKdSyoQdwdzczT0ZlTjGzycBkgG7duiXz0PWS06ENedNXVlm+fvpVKaiRiEhy1DcEDphZF3ffHzX1\nFEflRUDXuO1yo7KiaLlyeZXcfSGwEKCgoCCpAVMfid7o86avhJntz1zRvhv8eGsj10pEpOHqGwIr\ngInA7Ojnq3HlL5rZb4DziXUAb3T3cjM7YmaXA38D7gDmNajm6WLm4SrKqggGEZE0VGMImNkSYATQ\n0cwKgYeIvfkvM7NJwEfAzQDuvs3MlgHbgTJgiruXR4f6AbGRRm2A16OHiIikUI0h4O4TEqz6VoLt\nZwGzqijfDPSpU+1ERKRRNbhjOAhz8+Hwx1WseLHq7dt3U1+BiGQEhUBtHP646rb/KkYMAYnf6NVX\nICJpRnMHiYgETCEgIhIwhYCISMAUAiIiAVPHcFNKNGro1DqNHBKRJqYQaErVvclr5JCIpICag0RE\nAqYQEBEJmEJARCRg6hNoAH3PgIhkOoVAA1T7PQN1pfmGRCQFFALpQvMNiUgKKATiJZottH3qv+JS\nRKQxKATiJZotVESkmdLoIBGRgCkEREQCpuagdKdRQyLSiBQC6U6jhkSkEak5SEQkYAoBEZGAKQRE\nRAKmEBARCZhCQEQkYBod1AiaZHZRDR0VkSRQCDSCpM4umoiGjopIEqg5SEQkYAoBEZGAKQRERALW\noD4BM9sHHAXKgTJ3LzCzrwMvAXnAPuBmd/882n4GMCna/kfu/kZDXl+qoA5jEamDZHQMX+nun8Y9\nnw687e6zzWx69PwBM+sN3AJcApwPvGVmF7l7eRLqIKeow1hE6qAxRgeNBUZEy4uA/wc8EJUvdffj\nwF4z2w0MBv5/I9ShevoGMRERoOEh4MT+oi8HFrj7QqCzu++P1v8T6Bwt5wAb4vYtjMrOYGaTgckA\n3bo1whuzvkFMRARoeAgMc/ciMzsXeNPM/it+pbu7mXldDxqFyUKAgoKCOu8vIiK106DRQe5eFP0s\nBl4h1rxzwMy6AEQ/i6PNi4CucbvnRmUiIpIi9Q4BM2trZtmnloFrgQ+BFcDEaLOJwKvR8grgFjNr\nbWbdgR7Axvq+voiINFxDmoM6A6+Y2anjvOju/9fMNgHLzGwS8BFwM4C7bzOzZcB2oAyYopFBIiKp\nVe8QcPc9wKVVlB8CvpVgn1nArPq+pjSAPj8gIlXQBHKh0OcHRKQKmjZCRCRgCgERkYCpOagJJfqy\nmVPrkvaFMyIitaQQaELVvckn9Qtn6kIdxiJBUwiETh3GIkFTn4CISMAUAiIiAVMIiIgETCEgIhIw\nhYCISMA0OihNJPoMQco+P6ChoyJBUAikiURv9Cn7/ICGjooEQc1BIiIBUwiIiARMISAiEjD1CUjd\nqMNYpFlRCKS5tBs1pA5jkWZFIZDm0m7UUCK6QxDJSAoBSQ7dIYhkJHUMi4gETCEgIhIwNQdlqLTr\nME5EfQUiaa35h0CiN6AMlzEdxone6OfmKxxE0kAAIXA41TVoUhlzh6COZJG00PxDIDAZc4cgImlB\nIRCIjLlDSNSHcGpdVXcQc/Ph8Me1315EKigEApExdwjVvWlX149QVbOfmpZEaqQQCFzG3CGA/qoX\naQRNHgJmNgp4HGgB/B93n93UdZB/SfRGP3T2O5kTDoloeKpIjZo0BMysBfA74BqgENhkZivcfXtT\n1kNqljHNR9XRCCSRGjX1ncBgYLe77wEws6XAWEAhkCESNR/V5zgpu6OorvO5sV9XdyCSZpo6BHKA\nT+KeFwKXNXEdpAGS9cadqLmpKeR0eJz1M1MQQIk6tlNJwRS8tOwYNrPJwOToaYmZ7aznoTrar/k0\nSdVKlY6Q0eeQdvX/CLAZddol7c6hjqqp/4fwE2vSytRTpl8DaPpzuKA2GzV1CBQBXeOe50Zlp3H3\nhcDChr6YmW1294KGHieVMv0cMr3+kPnnkOn1B51DY2rqWUQ3AT3MrLuZ/S/gFmBFE9dBREQiTXon\n4O5lZnYf8AaxIaLPuvu2pqyDiIj8S5P3Cbj7KmBVE71cg5uU0kCmn0Om1x8y/xwyvf6gc2g05u6p\nroOIiKSIvllMRCRgzTIEzGyUme00s91mNj3V9akPM9tnZlvNbIuZbU51fWrDzJ41s2Iz+zCu7Otm\n9qaZ7Yp+np3KOtYkwTnMNLOi6FpsMbPRqaxjdcysq5mtMbPtZrbNzO6PyjPiOlRT/0y6BllmttHM\n/jM6h/8dlaflNWh2zUHR1BT/IG5qCmBCpk1NYWb7gAJ3z5ix0WY2HCgBnnP3PlHZfwCfufvsKJDP\ndvcHUlnP6iQ4h5lAibs/ksq61YaZdQG6uPv7ZpYNvAfcANxJBlyHaup/M5lzDQxo6+4lZtYKWAfc\nD9xIGl6D5ngnUDE1hbv/N3BqagppZO6+FvisUvFYYFG0vIjYf+i0leAcMoa773f396Plo8AOYp/U\nz4jrUE39M4bHlERPW0UPJ02vQXMMgaqmpsioX6KIA2+Z2XvRJ6gzVWd33x8t/xPonMrKNMAPzeyD\nqLkoLW7ja2JmeUB/4G9k4HWoVH/IoGtgZi3MbAtQDLzp7ml7DZpjCDQXw9y9H/BvwJSomSKjeazt\nMRPbH+cDFwL9gP3Ao6mtTs3MrB2wHJjq7kfi12XCdaii/hl1Ddy9PPr/mwsMNrM+ldanzTVojiFQ\nq6kp0p27F0U/i4FXiDVzZaIDUTvvqfbe4hTXp87c/UD0n/ok8DRpfi2idujlwAvu/qeoOGOuQ1X1\nz7RrcIq7fwGsAUaRptegOYZAxk9NYWZto04xzKwtcC3wYfV7pa0VwMRoeSLwagrrUi+n/uNGxpHG\n1yLqlHwG2OHuv4lblRHXIVH9M+wadDKzDtFyG2KDVP6LNL0GzW50EEA0fOwx/jU1xawUV6lOzOxC\nYn/9Q+xT3S9mwjmY2RJgBLHZEg8ADwF/BpYB3YhN4Hmzu6dtx2uCcxhBrBnCgX3APXFtu2nFzIYB\nfwW2Aiej4geJtaun/XWopv4TyJxr0JdYx28LYn9oL3P3X5rZOaThNWiWISAiIrXTHJuDRESklhQC\nIiIBUwiIiARMISAiEjCFgIhIwBQCIiIBUwiIiARMISAiErD/AVLKcUJhiDZIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb1addf7ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output = plt.hist([chi_squared_df2,chi_squared_df5], bins=50, histtype='step', \n",
    "                  label=['2 degrees of freedom','5 degrees of freedom'])\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last aspect of distributions that I want to talk about is the **modality**. So far, all of the distributions I've shown have a single high point, a peak. But what if we have multiple peaks? This distribution has two high points, so we call it **bimodal**. These are really interesting distributions and **happen regularly in data mining**. A useful insight is that **we can actually model these using two normal distributions with different parameters**. These are called **Gaussian Mixture Models** and are particularly useful when **clustering data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"binomial_distributions.png\" title=\"Binomial Distribution\" alt=\"binomial distribution.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that **a distribution is just a shape that describes the probability of a value being pulled when we sample a population**. And NumPy and SciPy each have a number of different distributions built in for us to be able to sample from. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use statistics in a lot of different ways in data science. In this lecture, I want to refresh your knowledge of **hypothesis testing**, which is a core data analysis activity behind experimentation. We're starting to see experimentation used more and more commonly outside of academic scientific, and in day to day business environments. Part of the reason for this is the rise of big data and web commerce. It's now easy to change your digital storefront and deliver a different experience to some of your customers, and then see how those customer actions might differ from one another. For instance, if you sell books, you might want to have one condition where the cover of the book is featured prominently on the web page and another condition where the focus is on the author and the reviews of the book. This is often called **A/B testing**. And while it's not unique to this time in history, it's now becoming so common that if you're using a website, you are undoubtedly part of an A/B test somewhere. This raises some interesting **ethical questions**.\n",
    "\n",
    "**A hypothesis is a statement that we can test**. I'll pull an example from my own research area of educational technology and learning analytics. Let's say that we have an expectation that when a new course is launched on a MOOC platform, the keenest students find out about it and all flock to it. Thus, we might expect that those students who sign up quite quickly after the course is launched will have higher performance than those students who signed up after the MOOC has been around for a while. In this example, we have samples from two different groups which we want to compare. The early sign ups and the late sign ups.\n",
    "\n",
    "When we do hypothesis testing, we **hold out that our hypothesis as the alternative and we create a second hypothesis called the null hypothesis**, which in this case would be that there is no difference between groups. We then examine the groups to determine whether this null hypothesis is true or not. \n",
    "If we find that there is a difference between groups, then we can reject the null hypothesis and we accept our alternative. There are subtleties in this description. We aren't saying that our hypothesis is true per se, but we're saying that there's evidence against the null hypothesis. So, we're more confident in our alternative hypothesis. \n",
    "\n",
    "Let's see an example of this. We can load a file called grades.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('grades.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the DataFrame inside, we see we have six different assignments, each with a submission time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801000000</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>\n",
       "      <td>64.813800</td>\n",
       "      <td>2015-12-13 17:06:10.750000000</td>\n",
       "      <td>51.491040</td>\n",
       "      <td>2015-12-14 12:25:12.056000000</td>\n",
       "      <td>41.932832</td>\n",
       "      <td>2015-12-29 14:25:22.594000000</td>\n",
       "      <td>36.929549</td>\n",
       "      <td>2015-12-28 01:29:55.901000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2015-12-29 14:46:06.628000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2016-01-05 01:06:59.546000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "4  5ECBEEB6-F1CE-80AE-3164-E45E99473FB4          64.813800   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "3  2016-04-30 06:50:39.801000000          68.824532   \n",
       "4  2015-12-13 17:06:10.750000000          51.491040   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "3  2016-04-30 17:20:38.727000000          61.942079   \n",
       "4  2015-12-14 12:25:12.056000000          41.932832   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3  2016-05-12 07:47:16.326000000          49.553663   \n",
       "4  2015-12-29 14:25:22.594000000          36.929549   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "4  2015-12-28 01:29:55.901000000          33.236594   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "4  2015-12-29 14:46:06.628000000          33.236594   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "4  2016-01-05 01:06:59.546000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it looks like there are just under 3,000 entries in this data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this lecture, let's segment this population into two pieces. Those who finish the first assignment by the end of December 2015 and those who finish it sometimes after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early = df[df['assignment1_submission'] <= '2015-12-31']\n",
    "late = df[df['assignment1_submission'] > '2015-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen, the pandas DataFrame object has a variety of statistical functions associated with it. If we call the mean function directly on the DataFrame, we see that each of the means for the assignments are calculated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the **datetime values are ignored as panda's knows this isn't a number, but an object type**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.972741\n",
       "assignment2_grade    67.252190\n",
       "assignment3_grade    61.129050\n",
       "assignment4_grade    54.157620\n",
       "assignment5_grade    48.634643\n",
       "assignment6_grade    43.838980\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the mean values for the late DataFrame as well, we get surprisingly similar numbers. There are slight differences, though. It looks like the end of the six assignments, the early users are doing better by about a percentage point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.017429\n",
       "assignment2_grade    66.370822\n",
       "assignment3_grade    60.023244\n",
       "assignment4_grade    54.058138\n",
       "assignment5_grade    48.599402\n",
       "assignment6_grade    43.844384\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, is this enough to go ahead and make some interventions to actually try and change something in the way we teach? When doing hypothesis testing, **we have to choose a significance level as a threshold** for how much of a chance we're willing to accept. **This significance level is typically called alpha**. It can vary greatly, depending on what you're going to do with the result and the amount of noise you expect in your data.\n",
    "\n",
    "For instance, **in social sciences research, a value of 0.05 or 0.01 is often used**, which indicates a tolerance for a probability between 5% and 1% of chance. **In a physics experiment where the conditions are much more controlled and thus, the burden of proof is much higher, you might expect to see alpha levels of 10 to the negative 5 or 100,000th of a percentage.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of the significance level from the perspective of interventions as well and this is something I run into regularly with my research. What am I going to do when I find out that two student populations are different? For instance, if I'm going to send an email nudge to encourage students to continue working on their homework, that's a pretty low-cost intervention. Emails are cheap and while I certainly don't want to annoy students, one extra email isn't going to ruin their day. But what if the intervention is a little more involved, like having our tutorial assistant followup with a student via phone? This is all of a sudden much more expensive for both the institution and for the student. So, I might want to ensure a higher burden of proof.\n",
    "\n",
    "So **the threshold you set for alpha depends on what you might do with the result, as well**. For this example, let's use a threshold of 0.05 for our alpha or 5%. Now, how do we actually test whether these means are different in Python? The **SciPy library contains a number of different statistical tests and forms a basis for hypothesis testing in Python**. A **t-test is one way to compare the means of two different populations**. In the SciPy library, the **ttest_ind function will compare two independent samples to see if they have different means**. I'm not going to go into the details of any of this statistical tests here, but instead, we'd recommend that you check out the Wikipedia page on particular test or consider taking a full statistics course if this is unfamiliar to you. But I do want to note that most statistical tests expect that **the data conforms to a certain distribution, a shape**. So, you shouldn't apply such tests blindly and should investigate your data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to compare the assignment grades for the first assignment between the two populations, we could generate a t-test by passing these two series into the ttest_ind function. **The result is a tuple with a test statistic and a p-value**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value here is much larger than our 0.05. So we cannot reject the null hypothesis, which is that the two populations are the same. In more lay terms, we would say that there's no statistically significant difference between these two sample means.\n",
    "\n",
    "Let's check with assignment two grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.3239868220912567, pvalue=0.18563824610067967)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment2_grade'], late['assignment2_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, that's much larger than 0.05 too. How about with assignment three? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.7116160037010733, pvalue=0.087101516341556676)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment3_grade'], late['assignment3_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that's much closer, but still beyond our threshold value. It's important to stop here and talk about **serious process problem with how we're handling this investigation** of the difference between these two populations. **When we set the alpha to be 0.05, we're saying that we expect it that there will be positive result, 5% of the time just due to chance**. As we run more and more t-tests, we're more likely to find a positive result just because of the number of t-tests we have run.\n",
    "\n",
    "When a data scientist runs many tests in this way, it's called **p-hacking or dredging and it's a serious methodological issue**. P-hacking results in **spurious correlations instead of generalizable results**. There are a couple of different ways you can deal with p-hacking. The first is called the **Bonferroni correction**. In this case, you simply tighten your alpha value, the threshold of significance, based on the number of tests you're running. So if you choose 0.05 with 1 test, and you want to run 3 tests, you reduce alpha by multiplying 0.05 by one-third to get a new value of 0.017. I personally find this approach to be very conservative. Another option is to **hold out some of your data for testing to see how generalizable your result is**. In this case, we might take half of our data for each of the two DataFrames, run our t-test with that, form specific hypothesis based on the result of these tests, then run very limited tests on the rest of the data.\n",
    "\n",
    "**This method is actually heavily used in machine learning when building predictive models**, where it's called cross fold validation. A final method which has come about is the **pre-registration** of your experiment. \n",
    "In this step, you would **outline what you expect to find and why, and describe the test that would backup a positive proof of this**. You register it with a third party, in academic circles, this is often a journal who determines whether it's a reasonable test to run or not. You then run your study and report the results, regardless as to whether they were positive or not. Here, there is a larger burden on connecting to existing theory since you need to convince reviewers that the experiment is likely to test fully a given hypothesis. In this lecture, we've discussed just some of the basics of hypothesis testing in Python. \n",
    "\n",
    "I introduced you to the SciPy library, which you can use for t-testing. We've discussed some of the practical issues which arise from looking for statistical significance. There's much more to learn about hypothesis testing. For instance, there are different tests used, depending on the shape of your data, and different ways to report results instead of just p-values, such as confidence intervals. But I hope this gives you a start to comparing the means of two different populations, which is a common task for data scientists, and we'll followup some of this work in the second course in this series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"p-hacking.png\" title=\"p-hacking slikde\" alt=\"p-hacking.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
